{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loding Dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {'Organic': 0, 'Recyclable': 1}\n",
    "\n",
    "    for label in os.listdir(data_dir):\n",
    "        if label in label_map:\n",
    "            for img_file in os.listdir(os.path.join(data_dir, label)):\n",
    "                img_path = os.path.join(data_dir, label, img_file)\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (224, 224))  \n",
    "                images.append(image)\n",
    "                labels.append(label_map[label])\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load the dataset\n",
    "X, y = load_data('dataset/train')\n",
    "\n",
    "# Normalize pixel values\n",
    "X = X / 255.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning Neural Network #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))  # 2 classes: organic and recyclable\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6970 - val_accuracy: 0.4286 - val_loss: 1.1102\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.6250 - loss: 1.6562 - val_accuracy: 0.5714 - val_loss: 2.2364\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 575ms/step - accuracy: 0.5000 - loss: 3.3336 - val_accuracy: 0.4286 - val_loss: 1.5510\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473ms/step - accuracy: 0.7083 - loss: 1.3198 - val_accuracy: 0.4286 - val_loss: 0.9841\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 505ms/step - accuracy: 0.5833 - loss: 0.6995 - val_accuracy: 0.8571 - val_loss: 0.5465\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - accuracy: 0.9583 - loss: 0.3690 - val_accuracy: 0.8571 - val_loss: 0.4967\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step - accuracy: 0.8750 - loss: 0.3534 - val_accuracy: 0.7143 - val_loss: 0.5183\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - accuracy: 0.9583 - loss: 0.2804 - val_accuracy: 0.4286 - val_loss: 0.9622\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - accuracy: 0.7917 - loss: 0.3726 - val_accuracy: 0.7143 - val_loss: 0.3892\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step - accuracy: 0.7917 - loss: 0.4016 - val_accuracy: 1.0000 - val_loss: 0.2273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x28de0146db0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save these Model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('waste_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Model result in Commnad Line using image #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\T'\n",
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_14888\\3877082635.py:17: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  result = classify_waste(\"Dataset\\Test\\Organic\\WhatsApp Image 2025-03-16 at 7.52.28 PM.jpeg\", model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Organic Waste\n"
     ]
    }
   ],
   "source": [
    "def classify_waste(Dataset, model, threshold=0.5):\n",
    "    image = cv2.imread(Dataset)\n",
    "    image = cv2.resize(image, (224, 224))  \n",
    "\n",
    "    # Normalize\n",
    "    image = np.expand_dims(image, axis=0) / 255.0  \n",
    "\n",
    "    prediction = model.predict(image)\n",
    "    class_index = np.argmax(prediction)\n",
    "\n",
    "    # Check if the prediction confidence is above the threshold\n",
    "    if np.max(prediction) < threshold:\n",
    "        return \"NO RECORD FOUND\"\n",
    "    else:\n",
    "        return \"Organic Waste\" if class_index == 0 else \"Recyclable Waste\"\n",
    "\n",
    "result = classify_waste(\"Dataset\\Test\\Organic\\WhatsApp Image 2025-03-16 at 7.52.28 PM.jpeg\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Model result in Commnad Line using Camera #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Classification Result: Recyclable\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Classification Result: Organic\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Classification Result: Recyclable\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model('waste_classification_model.h5')\n",
    "\n",
    "# Function to classify waste\n",
    "def classify_waste(image, model, threshold=0.5):\n",
    "    image_resized = cv2.resize(image, (224, 224))  \n",
    "    image_normalized = np.expand_dims(image_resized, axis=0) / 255.0 \n",
    "\n",
    "    prediction = model.predict(image_normalized)\n",
    "    class_index = np.argmax(prediction)\n",
    "\n",
    "    if np.max(prediction) < threshold:\n",
    "        return \"Unknown\"\n",
    "    else:\n",
    "        return \"Organic\" if class_index == 0 else \"Recyclable\"\n",
    "\n",
    "# Initialize the camera\n",
    "# Use 0 for the default camera\n",
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Camera Feed\", frame)\n",
    "\n",
    "    # Wait for the user to press 'c' to classify the current frame\n",
    "    key = cv2.waitKey(1)\n",
    "    if key & 0xFF == ord('c'):\n",
    "        result = classify_waste(frame, model)\n",
    "        print(f\"Classification Result: {result}\")\n",
    "\n",
    "    # Exit the loop when 'q' is pressed\n",
    "    if key & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Model result in Frame using camera #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Classification Result: Recyclable\n"
     ]
    }
   ],
   "source": [
    "model = load_model('waste_classification_model.h5')\n",
    "\n",
    "def classify_waste(image, model, threshold=0.5):\n",
    "    image_resized = cv2.resize(image, (224, 224))  \n",
    "    image_normalized = np.expand_dims(image_resized, axis=0) / 255.0  \n",
    "\n",
    "    prediction = model.predict(image_normalized)\n",
    "    class_index = np.argmax(prediction)\n",
    "\n",
    "    if np.max(prediction) < threshold:\n",
    "        return \"Unknown\"\n",
    "    else:\n",
    "        return \"Organic\" if class_index == 0 else \"Recyclable\"\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"Camera Feed\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key & 0xFF == ord('c'):\n",
    "        result = classify_waste(frame, model)\n",
    "        print(f\"Classification Result: {result}\")\n",
    "\n",
    "        result_window = np.zeros((200, 400, 3), dtype=np.uint8) \n",
    "        cv2.putText(result_window, f\"Result: {result}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.imshow(\"Classification Result\", result_window)\n",
    "\n",
    "        \n",
    "        cv2.waitKey(3000) \n",
    "        cv2.destroyWindow(\"Classification Result\")  \n",
    "\n",
    "    if key & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
